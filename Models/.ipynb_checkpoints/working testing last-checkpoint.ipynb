{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f943094-32b9-463f-9bb9-0217fbd59c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02048d3-7851-4df8-886c-6eeced1cc5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ YOLO model loaded: yolov8n.pt\n",
      "üöå Fast Bus Lane Monitoring System\n",
      "==================================================\n",
      "Features:\n",
      "  ‚úÖ Normal playback speed\n",
      "  ‚úÖ Detects stopped autorickshaws via person detection\n",
      "  ‚úÖ Enhanced bike/motorcycle detection\n",
      "  ‚úÖ Optimized processing\n",
      "\n",
      "üìã Choose an option:\n",
      "1. Mark bus lane region\n",
      "2. Run fast monitoring\n",
      "3. View statistics\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4):  1\n",
      "Frame number (default: 10):  25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reference frame extracted with 500 features\n",
      "üéØ Instructions:\n",
      "   1. Click 8 points to define the bus lane region\n",
      "   2. This region will be tracked throughout the video\n",
      "   3. Press 'q' when done, 'r' to reset\n",
      "‚úÖ Bus lane region marked with 8 points\n",
      "‚úÖ Region marked!\n",
      "\n",
      "üìã Choose an option:\n",
      "1. Mark bus lane region\n",
      "2. Run fast monitoring\n",
      "3. View statistics\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöå Starting fast monitoring...\n",
      "‚úÖ Reference frame extracted with 500 features\n",
      "üöå Fast Bus Lane Monitoring Started\n",
      "   üìä 1410 frames at 30 FPS\n",
      "   üéØ Enhanced detection for stopped vehicles\n",
      "   ‚å®Ô∏è  Press 'q' to quit, 'p' to pause\n",
      "\n",
      "üìä Final Statistics:\n",
      "   üöó Total Detections: 172\n",
      "   ‚úÖ Authorized: 11\n",
      "   ‚ùå Violations: 161\n",
      "\n",
      "üìã Choose an option:\n",
      "1. Mark bus lane region\n",
      "2. Run fast monitoring\n",
      "3. View statistics\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "class VideoRegionTracker:\n",
    "    def __init__(self):\n",
    "        self.points = []\n",
    "        self.reference_frame = None\n",
    "        self.reference_keypoints = None\n",
    "        self.reference_descriptors = None\n",
    "        self.window_name = \"Mark Region - Click 8 Points\"\n",
    "        self.points_file = \"tracked_region_points.json\"\n",
    "        \n",
    "        # Initialize ORB detector for feature matching\n",
    "        self.orb = cv2.ORB_create(nfeatures=500)  # Reduced features for speed\n",
    "        \n",
    "        # Initialize FLANN matcher\n",
    "        FLANN_INDEX_LSH = 6\n",
    "        index_params = dict(algorithm=FLANN_INDEX_LSH,\n",
    "                           table_number=6,\n",
    "                           key_size=12,\n",
    "                           multi_probe_level=1)\n",
    "        search_params = dict(checks=25)  # Reduced checks for speed\n",
    "        self.flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    def mouse_callback(self, event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN and len(self.points) < 8:\n",
    "            self.points.append([x, y])\n",
    "            \n",
    "            # Create a copy for display\n",
    "            display_img = self.reference_frame.copy()\n",
    "            \n",
    "            # Draw all points\n",
    "            for i, point in enumerate(self.points):\n",
    "                cv2.circle(display_img, tuple(point), 8, (0, 0, 255), -1)\n",
    "                cv2.putText(display_img, f\"{i+1}\", (point[0]+15, point[1]-15), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            \n",
    "            # Draw connecting lines\n",
    "            if len(self.points) > 1:\n",
    "                for i in range(len(self.points)-1):\n",
    "                    cv2.line(display_img, tuple(self.points[i]), tuple(self.points[i+1]), (0, 255, 0), 2)\n",
    "            \n",
    "            # Close polygon when 8 points are marked\n",
    "            if len(self.points) == 8:\n",
    "                cv2.line(display_img, tuple(self.points[-1]), tuple(self.points[0]), (0, 255, 0), 2)\n",
    "                cv2.putText(display_img, \"Press 'q' to save\", (50, 50),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow(self.window_name, display_img)\n",
    "    \n",
    "    def extract_reference_frame(self, video_path, frame_number=10):\n",
    "        \"\"\"Extract reference frame and compute features\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Error: Cannot open video file\")\n",
    "            return None\n",
    "        \n",
    "        # Go to specified frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number-1)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret:\n",
    "            print(f\"‚ùå Error: Cannot read frame {frame_number}\")\n",
    "            return None\n",
    "        \n",
    "        # Store reference frame\n",
    "        self.reference_frame = frame.copy()\n",
    "        \n",
    "        # Convert to grayscale for feature detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect keypoints and descriptors\n",
    "        self.reference_keypoints, self.reference_descriptors = self.orb.detectAndCompute(gray, None)\n",
    "        \n",
    "        if self.reference_descriptors is None:\n",
    "            print(\"‚ùå Error: No features detected in reference frame\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úÖ Reference frame extracted with {len(self.reference_keypoints)} features\")\n",
    "        return frame\n",
    "    \n",
    "    def mark_region(self, video_path, frame_number=10):\n",
    "        \"\"\"Mark region on reference frame\"\"\"\n",
    "        frame = self.extract_reference_frame(video_path, frame_number)\n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        self.points = []\n",
    "        \n",
    "        cv2.namedWindow(self.window_name)\n",
    "        cv2.setMouseCallback(self.window_name, self.mouse_callback)\n",
    "        \n",
    "        print(\"üéØ Instructions:\")\n",
    "        print(\"   1. Click 8 points to define the bus lane region\")\n",
    "        print(\"   2. This region will be tracked throughout the video\")\n",
    "        print(\"   3. Press 'q' when done, 'r' to reset\")\n",
    "        \n",
    "        cv2.imshow(self.window_name, frame)\n",
    "        \n",
    "        while True:\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            if key == ord('q') and len(self.points) == 8:\n",
    "                break\n",
    "            elif key == ord('r'):\n",
    "                self.points = []\n",
    "                cv2.imshow(self.window_name, frame)\n",
    "                print(\"üîÑ Points reset\")\n",
    "            elif key == 27:  # ESC\n",
    "                cv2.destroyAllWindows()\n",
    "                return None\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Save points and reference frame info\n",
    "        data = {\n",
    "            \"points\": self.points,\n",
    "            \"reference_frame\": frame_number\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.points_file, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "            \n",
    "            print(f\"‚úÖ Bus lane region marked with {len(self.points)} points\")\n",
    "            return self.points\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def find_homography(self, current_frame):\n",
    "        \"\"\"Find homography between reference frame and current frame\"\"\"\n",
    "        # Resize for faster processing\n",
    "        small_current = cv2.resize(current_frame, (640, 360))\n",
    "        small_ref = cv2.resize(self.reference_frame, (640, 360))\n",
    "        \n",
    "        gray_current = cv2.cvtColor(small_current, cv2.COLOR_BGR2GRAY)\n",
    "        gray_ref = cv2.cvtColor(small_ref, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect features in current frame\n",
    "        keypoints_current, descriptors_current = self.orb.detectAndCompute(gray_current, None)\n",
    "        keypoints_ref, descriptors_ref = self.orb.detectAndCompute(gray_ref, None)\n",
    "        \n",
    "        if descriptors_current is None or descriptors_ref is None or len(descriptors_current) < 10:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Match features\n",
    "            matches = self.flann.knnMatch(descriptors_ref, descriptors_current, k=2)\n",
    "            \n",
    "            # Filter good matches\n",
    "            good_matches = []\n",
    "            for match_pair in matches:\n",
    "                if len(match_pair) == 2:\n",
    "                    m, n = match_pair\n",
    "                    if m.distance < 0.75 * n.distance:  # Slightly relaxed for speed\n",
    "                        good_matches.append(m)\n",
    "            \n",
    "            if len(good_matches) < 8:  # Reduced requirement\n",
    "                return None\n",
    "            \n",
    "            # Extract matched points and scale back up\n",
    "            src_pts = np.float32([keypoints_ref[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([keypoints_current[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            \n",
    "            # Scale points back to original resolution\n",
    "            scale_x = current_frame.shape[1] / 640\n",
    "            scale_y = current_frame.shape[0] / 360\n",
    "            src_pts[:, :, 0] *= scale_x\n",
    "            src_pts[:, :, 1] *= scale_y\n",
    "            dst_pts[:, :, 0] *= scale_x\n",
    "            dst_pts[:, :, 1] *= scale_y\n",
    "            \n",
    "            # Find homography\n",
    "            homography, mask = cv2.findHomography(src_pts, dst_pts, \n",
    "                                                cv2.RANSAC, 8.0)  # Relaxed threshold\n",
    "            \n",
    "            return homography\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def transform_points(self, points, homography):\n",
    "        \"\"\"Transform points using homography\"\"\"\n",
    "        if homography is None:\n",
    "            return points\n",
    "        \n",
    "        # Convert points to the right format\n",
    "        pts = np.float32(points).reshape(-1, 1, 2)\n",
    "        \n",
    "        # Transform points\n",
    "        transformed_pts = cv2.perspectiveTransform(pts, homography)\n",
    "        \n",
    "        # Convert back to list format\n",
    "        return transformed_pts.reshape(-1, 2).astype(int).tolist()\n",
    "    \n",
    "    def draw_region(self, frame, points, color=(0, 255, 0), thickness=2):\n",
    "        \"\"\"Draw the tracked region - simplified for speed\"\"\"\n",
    "        if len(points) < 3:\n",
    "            return\n",
    "        \n",
    "        pts = np.array(points, np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        \n",
    "        # Draw border only (no fill for speed)\n",
    "        cv2.polylines(frame, [pts], True, color, thickness)\n",
    "        \n",
    "        # Draw corner points\n",
    "        for point in points:\n",
    "            cv2.circle(frame, tuple(point), 3, color, -1)\n",
    "\n",
    "class EnhancedBusLaneMonitor(VideoRegionTracker):\n",
    "    def __init__(self, model_path=\"yolov8n.pt\"):\n",
    "        super().__init__()\n",
    "        # Load YOLO model\n",
    "        try:\n",
    "            self.model = YOLO(model_path)\n",
    "            self.model.overrides['verbose'] = False\n",
    "            print(f\"‚úÖ YOLO model loaded: {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading YOLO model: {e}\")\n",
    "            self.model = None\n",
    "        \n",
    "        # Vehicle tracking statistics\n",
    "        self.violation_count = 0\n",
    "        self.authorized_count = 0\n",
    "        self.total_detections = 0\n",
    "        self.frame_violations = []\n",
    "        \n",
    "    def point_in_polygon(self, point, polygon):\n",
    "        \"\"\"Check if point is inside polygon using ray casting\"\"\"\n",
    "        x, y = point\n",
    "        n = len(polygon)\n",
    "        inside = False\n",
    "        \n",
    "        j = n - 1\n",
    "        for i in range(n):\n",
    "            if ((polygon[i][1] > y) != (polygon[j][1] > y)) and \\\n",
    "               (x < (polygon[j][0] - polygon[i][0]) * (y - polygon[i][1]) / (polygon[j][1] - polygon[i][1]) + polygon[i][0]):\n",
    "                inside = not inside\n",
    "            j = i\n",
    "        return inside\n",
    "    \n",
    "    def detect_stopped_vehicles(self, frame, region_points):\n",
    "        \"\"\"Special detection for stopped autorickshaws and bikes using person detection\"\"\"\n",
    "        if self.model is None:\n",
    "            return []\n",
    "        \n",
    "        # Get region mask for cropping\n",
    "        mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "        pts = np.array(region_points, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.fillPoly(mask, [pts], 255)\n",
    "        \n",
    "        # Get bounding box of region for cropping\n",
    "        x, y, w, h = cv2.boundingRect(pts)\n",
    "        \n",
    "        # Add padding\n",
    "        pad = 20\n",
    "        x = max(0, x - pad)\n",
    "        y = max(0, y - pad)\n",
    "        w = min(frame.shape[1] - x, w + 2*pad)\n",
    "        h = min(frame.shape[0] - y, h + 2*pad)\n",
    "        \n",
    "        # Crop frame to region\n",
    "        cropped_frame = frame[y:y+h, x:x+w]\n",
    "        cropped_mask = mask[y:y+h, x:x+w]\n",
    "        \n",
    "        # Run detection on cropped frame with very low confidence\n",
    "        results = self.model(cropped_frame, \n",
    "                           conf=0.15,  # Very low confidence\n",
    "                           iou=0.3,\n",
    "                           classes=[0, 1, 2, 3, 5, 7, 9, 14],  # person, bicycle, car, motorcycle, bus, truck, boat, skateboard\n",
    "                           verbose=False)\n",
    "        \n",
    "        detections = []\n",
    "        \n",
    "        for r in results:\n",
    "            if r.boxes is not None:\n",
    "                for box in r.boxes:\n",
    "                    cls = int(box.cls[0])\n",
    "                    label = r.names[cls]\n",
    "                    confidence = float(box.conf[0])\n",
    "                    \n",
    "                    # Convert coordinates back to full frame\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    x1, y1, x2, y2 = x1 + x, y1 + y, x2 + x, y2 + y\n",
    "                    \n",
    "                    center_x = (x1 + x2) // 2\n",
    "                    center_y = (y1 + y2) // 2\n",
    "                    \n",
    "                    # Check if detection is in region\n",
    "                    if self.point_in_polygon((center_x, center_y), region_points):\n",
    "                        \n",
    "                        width = x2 - x1\n",
    "                        height = y2 - y1\n",
    "                        aspect_ratio = width / height if height > 0 else 0\n",
    "                        area = width * height\n",
    "                        \n",
    "                        detected_vehicle = None\n",
    "                        \n",
    "                        # Enhanced detection logic\n",
    "                        if label == \"person\":\n",
    "                            # Person standing near/on vehicle - likely autorickshaw driver or bike rider\n",
    "                            if height > 80:  # Tall person detection\n",
    "                                # Look for vehicle-like shapes around the person\n",
    "                                detected_vehicle = \"autorickshaw\"\n",
    "                                confidence = min(confidence * 1.2, 0.9)\n",
    "                        \n",
    "                        elif label == \"bicycle\" or label == \"motorcycle\":\n",
    "                            detected_vehicle = \"motorcycle\"\n",
    "                            \n",
    "                        elif label == \"car\":\n",
    "                            # Many autorickshaws detected as cars\n",
    "                            if 1.0 < aspect_ratio < 2.2 and 3000 < area < 12000:\n",
    "                                detected_vehicle = \"autorickshaw\"\n",
    "                                confidence = confidence * 0.9\n",
    "                            else:\n",
    "                                detected_vehicle = \"car\"\n",
    "                        \n",
    "                        elif label == \"truck\":\n",
    "                            if aspect_ratio > 2.0 and area > 15000:\n",
    "                                detected_vehicle = \"bus\"\n",
    "                            elif 1.2 < aspect_ratio < 2.0 and area < 15000:\n",
    "                                detected_vehicle = \"autorickshaw\"\n",
    "                            else:\n",
    "                                detected_vehicle = \"truck\"\n",
    "                        \n",
    "                        elif label == \"bus\":\n",
    "                            detected_vehicle = \"bus\"\n",
    "                        \n",
    "                        elif label == \"boat\":  # Sometimes autorickshaws detected as boats\n",
    "                            detected_vehicle = \"autorickshaw\"\n",
    "                            confidence = confidence * 0.7\n",
    "                        \n",
    "                        if detected_vehicle:\n",
    "                            self.total_detections += 1\n",
    "                            \n",
    "                            # Authorization logic\n",
    "                            if detected_vehicle == \"bus\":\n",
    "                                status = \"AUTHORIZED\"\n",
    "                                color = (0, 255, 0)  # Green\n",
    "                                self.authorized_count += 1\n",
    "                            else:\n",
    "                                status = \"UNAUTHORIZED\"\n",
    "                                color = (0, 0, 255)  # Red\n",
    "                                self.violation_count += 1\n",
    "                            \n",
    "                            detections.append({\n",
    "                                'bbox': (x1, y1, x2, y2),\n",
    "                                'center': (center_x, center_y),\n",
    "                                'label': detected_vehicle,\n",
    "                                'confidence': confidence,\n",
    "                                'status': status,\n",
    "                                'color': color,\n",
    "                                'original_label': label\n",
    "                            })\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def draw_detections(self, frame, detections):\n",
    "        \"\"\"Draw vehicle detections - simplified for speed\"\"\"\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "            label = detection['label']\n",
    "            confidence = detection['confidence']\n",
    "            status = detection['status']\n",
    "            color = detection['color']\n",
    "            \n",
    "            # Draw bounding box\n",
    "            thickness = 3 if status == \"UNAUTHORIZED\" else 2\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n",
    "            \n",
    "            # Simple status text\n",
    "            status_text = f\"{status}\"\n",
    "            vehicle_text = f\"{label.upper()}\"\n",
    "            \n",
    "            # Draw status\n",
    "            cv2.putText(frame, status_text, (x1, y1 - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            \n",
    "          \n",
    "    \n",
    "    def draw_simple_stats(self, frame, detections, frame_num):\n",
    "        \"\"\"Draw simplified statistics for speed\"\"\"\n",
    "        violations = sum(1 for d in detections if d['status'] == \"UNAUTHORIZED\")\n",
    "        authorized = sum(1 for d in detections if d['status'] == \"AUTHORIZED\")\n",
    "        \n",
    "        # Simple stats overlay\n",
    "        stats_text = [\n",
    "            f\"Frame: {frame_num}\",\n",
    "            f\"Violations: {violations}\",\n",
    "            f\"Authorized: {authorized}\",\n",
    "            f\"Total V: {self.violation_count}\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(stats_text):\n",
    "            color = (0, 0, 255) if \"Violations\" in text else (255, 255, 255)\n",
    "            cv2.putText(frame, text, (10, 30 + i*25),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "    \n",
    "    def play_video_with_bus_detection(self, video_path):\n",
    "        \"\"\"Play video at normal speed with optimized detection\"\"\"\n",
    "        # Load saved region data\n",
    "        if not os.path.exists(self.points_file):\n",
    "            print(\"‚ùå No marked region found. Please mark region first.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(self.points_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            original_points = data[\"points\"]\n",
    "            reference_frame_num = data[\"reference_frame\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Re-extract reference frame for tracking\n",
    "        if self.extract_reference_frame(video_path, reference_frame_num) is None:\n",
    "            return\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Error: Cannot open video file\")\n",
    "            return\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        # Reset counters\n",
    "        self.violation_count = 0\n",
    "        self.authorized_count = 0\n",
    "        self.total_detections = 0\n",
    "        \n",
    "        print(f\"üöå Fast Bus Lane Monitoring Started\")\n",
    "        print(f\"   üìä {total_frames} frames at {fps} FPS\")\n",
    "        print(\"   üéØ Enhanced detection for stopped vehicles\")\n",
    "        print(\"   ‚å®Ô∏è  Press 'q' to quit, 'p' to pause\")\n",
    "        \n",
    "        paused = False\n",
    "        frame_count = 0\n",
    "        current_region_points = original_points\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        region_color = (0, 255, 0)\n",
    "        \n",
    "        while True:\n",
    "            if not paused:\n",
    "                ret, frame = cap.read()\n",
    "                frame_count += 1\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"üìΩÔ∏è Video finished\")\n",
    "                    break\n",
    "                \n",
    "                current_frame_num = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "                \n",
    "                # Simplified tracking - update every 10 frames for speed\n",
    "                if frame_count % 10 == 0:\n",
    "                    homography = self.find_homography(frame)\n",
    "                    if homography is not None:\n",
    "                        transformed_points = self.transform_points(original_points, homography)\n",
    "                        if len(transformed_points) == len(original_points):\n",
    "                            current_region_points = transformed_points\n",
    "                            region_color = (0, 255, 0)  # Green\n",
    "                        else:\n",
    "                            region_color = (0, 165, 255)  # Orange\n",
    "                    else:\n",
    "                        region_color = (0, 165, 255)  # Orange\n",
    "                \n",
    "                # Draw bus lane region (simplified)\n",
    "                self.draw_region(frame, current_region_points, region_color, 2)\n",
    "                \n",
    "                # Add bus lane label\n",
    "                cv2.putText(frame, \"BUS LANE\", \n",
    "                           (current_region_points[0][0], current_region_points[0][1] - 10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, region_color, 2)\n",
    "                \n",
    "                # Detect vehicles (every frame for better detection)\n",
    "                detections = self.detect_stopped_vehicles(frame, current_region_points)\n",
    "                \n",
    "                # Draw detections\n",
    "                self.draw_detections(frame, detections)\n",
    "                \n",
    "                # Draw simple statistics\n",
    "                self.draw_simple_stats(frame, detections, current_frame_num)\n",
    "            \n",
    "            cv2.imshow(\"üöå Fast Bus Lane Monitor\", frame)\n",
    "            \n",
    "            # Normal video speed - calculate appropriate delay\n",
    "            delay = max(1, int(1000 / fps) - 5)  # Slightly faster than real-time\n",
    "            key = cv2.waitKey(delay) & 0xFF\n",
    "            \n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):\n",
    "                paused = not paused\n",
    "                print(\"‚è∏Ô∏è Paused\" if paused else \"‚ñ∂Ô∏è Resumed\")\n",
    "        \n",
    "        # Final statistics\n",
    "        print(f\"\\nüìä Final Statistics:\")\n",
    "        print(f\"   üöó Total Detections: {self.total_detections}\")\n",
    "        print(f\"   ‚úÖ Authorized: {self.authorized_count}\")\n",
    "        print(f\"   ‚ùå Violations: {self.violation_count}\")\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Initialize the monitor\n",
    "    monitor = EnhancedBusLaneMonitor(\"yolov8n.pt\")\n",
    "    video_path = \"testingfix.mp4\"\n",
    "    \n",
    "    print(\"üöå Fast Bus Lane Monitoring System\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Features:\")\n",
    "    print(\"  ‚úÖ Normal playback speed\")\n",
    "    print(\"  ‚úÖ Detects stopped autorickshaws via person detection\")\n",
    "    print(\"  ‚úÖ Enhanced bike/motorcycle detection\")\n",
    "    print(\"  ‚úÖ Optimized processing\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nüìã Choose an option:\")\n",
    "        print(\"1. Mark bus lane region\")\n",
    "        print(\"2. Run fast monitoring\")\n",
    "        print(\"3. View statistics\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"Enter choice (1-4): \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            frame_num = input(\"Frame number (default: 10): \").strip()\n",
    "            try:\n",
    "                frame_num = int(frame_num) if frame_num else 10\n",
    "            except:\n",
    "                frame_num = 10\n",
    "            \n",
    "            points = monitor.mark_region(video_path, frame_num)\n",
    "            if points:\n",
    "                print(\"‚úÖ Region marked!\")\n",
    "                \n",
    "        elif choice == \"2\":\n",
    "            print(\"\\nüöå Starting fast monitoring...\")\n",
    "            monitor.play_video_with_bus_detection(video_path)\n",
    "            \n",
    "        elif choice == \"3\":\n",
    "            print(f\"\\nüìä Statistics:\")\n",
    "            print(f\"   Detections: {monitor.total_detections}\")\n",
    "            print(f\"   Violations: {monitor.violation_count}\")\n",
    "                \n",
    "        elif choice == \"4\":\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Invalid choice\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fea517-35a0-49da-91c6-a51b61a00c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
